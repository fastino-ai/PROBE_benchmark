"""
PersonaSim Evaluation UI

A Streamlit application for:
1. Generating evaluation examples
2. Annotating examples (marking true positives, identifying bottlenecks, selecting actions)
3. Scoring annotations against ground truth with IR metrics
4. Batch operations for multiple examples
"""

import streamlit as st
import json
from pathlib import Path
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Set
import os
from dotenv import load_dotenv
import numpy as np
import concurrent.futures

from data_generation.data.linkedin_profile import load_linkedin_personas
from annotation.prepare_for_annotation import prepare_single_example
from annotation.annotation_format import Annotation, ActionSelection, save_annotation
from evaluation.scoring import ChecklistScorer
from data_generation.utils.clients.openai_client import get_openai_client
from run import PROBEPipeline
from configs.config_schema import DataGenerationConfig, DifficultyLevel, ExecutionMode


load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


# Page configuration
st.set_page_config(
    page_title="PersonaSim Evaluation UI",
    page_icon="🎯",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Initialize session state
if "personas" not in st.session_state:
    st.session_state.personas = None
if "generated_examples" not in st.session_state:
    st.session_state.generated_examples = {}
if "loaded_examples" not in st.session_state:
    st.session_state.loaded_examples = {}
if "annotations" not in st.session_state:
    st.session_state.annotations = {}
if "scores" not in st.session_state:
    st.session_state.scores = {}


def load_personas_cached():
    """Load personas with caching."""
    if st.session_state.personas is None:
        with st.spinner("Loading LinkedIn personas..."):
            st.session_state.personas = load_linkedin_personas(limit=100)
    return st.session_state.personas


def generate_single_example(
    persona,
    difficulty: str,
    generate_distractors: bool = True,
    distractor_k: int = 20,
):
    """
    Generate a complete evaluation example using the actual PROBEPipeline.

    Args:
        persona: LinkedInPersona object (will be temporarily saved to use as index)
        difficulty: Difficulty level as string ("easy", "medium", "hard")
        generate_distractors: Whether to generate distractors
        distractor_k: Number of distractors to generate

    Returns:
        Dict with persona, world_model, bottleneck, checklist, true_positives, distractors
    """
    import tempfile
    import shutil

    # Create a temporary output directory
    temp_dir = Path(tempfile.mkdtemp(prefix="probe_ui_gen_"))

    try:
        # Temporarily save persona list with just this one persona
        # We need to make the pipeline think this persona is at index 0
        personas_backup = None
        personas_file = Path("data_generation/data/linkedin_personas.json")

        # Backup original personas if needed
        if personas_file.exists():
            import json

            with open(personas_file, "r") as f:
                personas_backup = json.load(f)

        # Create config for single persona generation
        config = DataGenerationConfig(
            mode=ExecutionMode.TEST,
            count=1,
            difficulty=DifficultyLevel(difficulty),
            generate_distractors=generate_distractors,
            distractor_count=distractor_k,
            max_workers=1,
            parallel=False,
            start_persona_index=0,
            output_directory=temp_dir,
            coordinated_generation=True,
        )

        # Create and run pipeline
        # We'll monkey-patch the personas list to use our selected one
        pipeline = PROBEPipeline.__new__(PROBEPipeline)
        pipeline.config = config
        pipeline.setup()

        # Override the personas list with our selected persona
        pipeline.personas = [persona]

        # Run the pipeline
        pipeline.run()

        # Extract the generated result from the output files
        # The pipeline saves files in inputs/ and outputs/ directories
        output_dir = (
            list(temp_dir.glob("*_test"))[0]
            if list(temp_dir.glob("*_test"))
            else temp_dir
        )
        outputs_dir = output_dir / "outputs"

        # Find the generated output file
        output_files = list(outputs_dir.glob("*_output.json"))
        if not output_files:
            raise RuntimeError("No output files generated by pipeline")

        # Load the generated example
        import json

        with open(output_files[0], "r") as f:
            output_data = json.load(f)

        # Load corresponding input file
        inputs_dir = output_dir / "inputs"
        input_files = list(inputs_dir.glob("*_input.json"))
        input_data = None
        if input_files:
            with open(input_files[0], "r") as f:
                input_data = json.load(f)

        # Extract data into the format expected by the UI
        result = {
            "persona": persona,
            "world_model": output_data.get("world_model"),
            "difficulty": difficulty,
            "bottleneck": output_data.get("bottleneck"),
            "bottlenecks": [output_data.get("bottleneck")],
            "checklist": output_data.get("checklist"),
            "checklists": [output_data.get("checklist")],
            "true_positives": [
                dp
                for dp in (input_data.get("data_points", []) if input_data else [])
                if dp.get("id") in output_data.get("true_positive_ids", [])
            ],
            "distractors": [
                dp
                for dp in (input_data.get("data_points", []) if input_data else [])
                if dp.get("id") in output_data.get("distractor_ids", [])
            ],
        }

        return result

    finally:
        # Clean up temporary directory
        if temp_dir.exists():
            shutil.rmtree(temp_dir)

        # Restore original personas file if we backed it up
        if personas_backup is not None:
            with open(personas_file, "w") as f:
                json.dump(personas_backup, f, indent=2)


def get_llm_function(model_name: str):
    """Create LLM function for generation."""

    client = get_openai_client()

    def llm_generate_func(prompt: str) -> str:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at generating evaluation data for proactive AI systems. Always respond with valid JSON.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=1,
            response_format={"type": "json_object"},
        )
        return response.choices[0].message.content

    return llm_generate_func


def calculate_ir_metrics(
    predicted_indices: Set[int], true_indices: Set[int], total_items: int
) -> Dict[str, float]:
    """Calculate information retrieval metrics."""
    if not true_indices:
        return {
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "accuracy": 0.0,
        }

    true_positives = len(predicted_indices & true_indices)
    false_positives = len(predicted_indices - true_indices)
    false_negatives = len(true_indices - predicted_indices)

    precision = true_positives / len(predicted_indices) if predicted_indices else 0.0
    recall = true_positives / len(true_indices) if true_indices else 0.0
    f1 = (
        2 * (precision * recall) / (precision + recall)
        if (precision + recall) > 0
        else 0.0
    )

    # Calculate accuracy considering true negatives
    true_negatives = total_items - len(predicted_indices | true_indices)
    accuracy = (
        (true_positives + true_negatives) / total_items if total_items > 0 else 0.0
    )

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "true_negatives": true_negatives,
    }


def calculate_ndcg(
    predicted_indices: List[int], true_indices: Set[int], k: int = None
) -> float:
    """Calculate Normalized Discounted Cumulative Gain."""
    if not true_indices or not predicted_indices:
        return 0.0

    if k is not None:
        predicted_indices = predicted_indices[:k]

    # Calculate DCG
    dcg = 0.0
    for i, idx in enumerate(predicted_indices):
        if idx in true_indices:
            dcg += 1.0 / np.log2(i + 2)  # i+2 because positions start at 1

    # Calculate IDCG (ideal DCG)
    idcg = 0.0
    for i in range(min(len(true_indices), len(predicted_indices))):
        idcg += 1.0 / np.log2(i + 2)

    return dcg / idcg if idcg > 0 else 0.0


def load_batch_examples(directory: Path) -> Dict[str, Any]:
    """Load all examples from a batch directory."""
    examples = {}
    if directory.exists() and directory.is_dir():
        for json_file in directory.glob("*.json"):
            if "summary" not in json_file.name:
                try:
                    with open(json_file, "r") as f:
                        data = json.load(f)
                        examples[json_file.stem] = data
                except Exception as e:
                    st.error(f"Error loading {json_file.name}: {e}")
    return examples


# Sidebar navigation
st.sidebar.title("🎯 PersonaSim Evaluation")
page = st.sidebar.radio(
    "Navigation",
    [
        "Generate Examples",
        "Annotate Examples",
        "Score Annotations",
        "Batch Scoring",
        "LLM Scoring Validation",
    ],
)

# Main content
if page == "Generate Examples":
    st.title("Generate Evaluation Examples")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Configuration")

        # Load personas
        personas = load_personas_cached()

        # Persona selection
        persona_names = [f"{p.name} - {p.occupation}" for p in personas]
        selected_idx = st.selectbox(
            "Select Persona",
            range(len(persona_names)),
            format_func=lambda x: persona_names[x],
        )

        selected_persona = personas[selected_idx]

        # Display persona details
        with st.expander("Persona Details", expanded=True):
            st.write(f"**Name:** {selected_persona.name}")
            st.write(f"**Occupation:** {selected_persona.occupation}")
            st.write(f"**Location:** {selected_persona.location}")
            st.write(f"**About:** {selected_persona.about}")

        # Generation parameters
        difficulty = st.select_slider(
            "Difficulty Level", options=["easy", "medium", "hard"], value="medium"
        )

        generate_distractors = st.checkbox("Generate Distractors", value=True)

        if generate_distractors:
            distractor_count = st.slider(
                "Number of Distractors",
                min_value=5,
                max_value=30,
                value=20,
            )
        else:
            distractor_count = 0

        st.info("💡 Uses the multi-LLM client from your configuration")

        # Generate button
        if st.button("🚀 Generate Example", type="primary"):
            with st.spinner("Generating evaluation example..."):
                try:
                    # Generate example using the actual pipeline
                    result = generate_single_example(
                        persona=selected_persona,
                        difficulty=difficulty,
                        generate_distractors=generate_distractors,
                        distractor_k=distractor_count,
                    )

                    # Store in session state
                    example_id = f"example_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    st.session_state.generated_examples[example_id] = result

                    st.success(f"✅ Generated example: {example_id}")

                    # Display summary
                    st.subheader("Generation Summary")
                    st.write(f"**Bottlenecks:** {len(result['bottlenecks'])}")
                    st.write(f"**Checklists:** {len(result['checklists'])}")
                    st.write(f"**True Positives:** {len(result['true_positives'])}")
                    if "distractors" in result:
                        st.write(f"**Distractors:** {len(result['distractors'])}")

                except Exception as e:
                    st.error(f"❌ Generation failed: {str(e)}")
                    import traceback

                    st.error(traceback.format_exc())

    with col2:
        st.subheader("Generated Examples")

        if st.session_state.generated_examples:
            for example_id, example_data in st.session_state.generated_examples.items():
                with st.expander(example_id):
                    persona = example_data["persona"]
                    st.write(
                        f"**Persona:** {persona.name if hasattr(persona, 'name') else persona.get('name', 'Unknown')}"
                    )
                    st.write(
                        f"**Difficulty:** {example_data['difficulty'].value if hasattr(example_data['difficulty'], 'value') else example_data['difficulty']}"
                    )
                    st.write(f"**Bottlenecks:** {len(example_data['bottlenecks'])}")

                    # Download buttons
                    col1, col2 = st.columns(2)
                    with col1:
                        # Full example
                        st.download_button(
                            "📥 Full Example",
                            data=json.dumps(example_data, indent=2, default=str),
                            file_name=f"{example_id}_full.json",
                            mime="application/json",
                        )

                    with col2:
                        # Annotation version
                        input_data, output_data = prepare_single_example(example_data)
                        st.download_button(
                            "📝 For Annotation",
                            data=json.dumps(input_data, indent=2, default=str),
                            file_name=f"{example_id}_input.json",
                            mime="application/json",
                        )
        else:
            st.info("No examples generated yet")

elif page == "Annotate Examples":
    st.title("Annotate Evaluation Examples")

    # Load examples
    st.subheader("Load Example")

    load_method = st.radio(
        "Load Method",
        ["From Input/Output Directories", "Upload File", "From Generated"],
    )

    input_data = None
    output_data = None
    example_id = None

    if load_method == "From Input/Output Directories":
        # Enhanced batch annotation interface with directory browser
        st.subheader("📂 Select Batch Directories")

        col1, col2 = st.columns(2)

        with col1:
            st.write("### Input Directory")
            st.write("Select directory containing *_input.json files")

            # Text input for manual path entry
            input_dir_path = st.text_input(
                "Input directory path",
                value="batch_examples/inputs/",
                placeholder="e.g., batch_examples/inputs/batch_20250919_115247_40_examples_hard",
                help="Enter the path to your input directory",
                key="batch_input_path",
            )

            # Try to list available batch directories
            if input_dir_path:
                input_base = Path(input_dir_path)
                if input_base.exists() and input_base.is_dir():
                    # If it's a base directory, show subdirectories
                    subdirs = [d.name for d in input_base.iterdir() if d.is_dir()]
                    if subdirs and not any(input_base.glob("*_input.json")):
                        st.write("**Available batch directories:**")
                        selected_subdir = st.selectbox(
                            "Select batch",
                            [""] + sorted(subdirs),
                            key="input_batch_select",
                        )
                        if selected_subdir:
                            input_dir_path = str(input_base / selected_subdir)

                    # Check for input files
                    input_dir = Path(input_dir_path)
                    if input_dir.exists():
                        input_files = list(input_dir.glob("*_input.json"))
                        if input_files:
                            st.success(f"✓ Found {len(input_files)} input files")
                            st.session_state.selected_input_dir = str(input_dir)
                        else:
                            st.warning("No *_input.json files found in this directory")
                            st.session_state.selected_input_dir = None
                    else:
                        st.error("Directory does not exist")
                        st.session_state.selected_input_dir = None
                else:
                    st.error("Invalid directory path")
                    st.session_state.selected_input_dir = None

        with col2:
            st.write("### Output Directory")
            st.write(
                "Select directory containing *_output.json files (for ground truth)"
            )

            # Text input for manual path entry
            output_dir_path = st.text_input(
                "Output directory path",
                value="batch_examples/outputs/",
                placeholder="e.g., batch_examples/outputs/batch_20250919_115247_40_examples_hard",
                help="Enter the path to your output directory",
                key="batch_output_path",
            )

            # Try to list available batch directories
            if output_dir_path:
                output_base = Path(output_dir_path)
                if output_base.exists() and output_base.is_dir():
                    # If it's a base directory, show subdirectories
                    subdirs = [d.name for d in output_base.iterdir() if d.is_dir()]
                    if subdirs and not any(output_base.glob("*_output.json")):
                        st.write("**Available batch directories:**")
                        selected_subdir = st.selectbox(
                            "Select batch",
                            [""] + sorted(subdirs),
                            key="output_batch_select",
                        )
                        if selected_subdir:
                            output_dir_path = str(output_base / selected_subdir)

                    # Check for output files
                    output_dir = Path(output_dir_path)
                    if output_dir.exists():
                        output_files = list(output_dir.glob("*_output.json"))
                        if output_files:
                            st.success(f"✓ Found {len(output_files)} output files")
                            st.session_state.selected_output_dir = str(output_dir)
                        else:
                            st.warning("No *_output.json files found in this directory")
                            st.session_state.selected_output_dir = None
                    else:
                        st.error("Directory does not exist")
                        st.session_state.selected_output_dir = None
                else:
                    st.error("Invalid directory path")
                    st.session_state.selected_output_dir = None

        # Enhanced example selection and annotation interface
        if (
            hasattr(st.session_state, "selected_input_dir")
            and st.session_state.selected_input_dir
        ):
            input_dir = Path(st.session_state.selected_input_dir)
            output_dir = (
                Path(st.session_state.selected_output_dir)
                if hasattr(st.session_state, "selected_output_dir")
                and st.session_state.selected_output_dir
                else None
            )

            # Load available examples
            input_files = sorted(list(input_dir.glob("*_input.json")))
            example_ids = [f.stem.replace("_input", "") for f in input_files]

            if example_ids:
                st.subheader(f"📋 Available Examples ({len(example_ids)} total)")

                # Example selection with batch options
                col1, col2 = st.columns([2, 1])

                with col1:
                    # Single or multiple selection mode
                    annotation_mode = st.radio(
                        "Annotation Mode:",
                        ["Single Example", "Batch Mode"],
                        horizontal=True,
                        help="Choose single example or batch annotation mode",
                    )

                    if annotation_mode == "Single Example":
                        # Original single example selection
                        selected_example_id = st.selectbox(
                            "Select Example", example_ids
                        )
                        selected_examples = (
                            [selected_example_id] if selected_example_id else []
                        )
                    else:
                        # Multi-select for batch annotation
                        selected_examples = st.multiselect(
                            "Select examples to annotate:",
                            options=example_ids,
                            help="Choose which examples you want to annotate in this session",
                        )

                with col2:
                    if annotation_mode == "Batch Mode":
                        # Quick selection options for batch mode
                        st.write("**Quick Select:**")
                        if st.button("📝 First 5"):
                            st.session_state.batch_selected = example_ids[:5]
                            st.rerun()
                        if st.button("🎲 Random 5"):
                            import random

                            st.session_state.batch_selected = random.sample(
                                example_ids, min(5, len(example_ids))
                            )
                            st.rerun()
                        if st.button("📄 All"):
                            st.session_state.batch_selected = example_ids
                            st.rerun()
                        if st.button("🗑️ Clear"):
                            st.session_state.batch_selected = []
                            st.rerun()

                        # Use session state for selections if available
                        if hasattr(st.session_state, "batch_selected"):
                            selected_examples = st.session_state.batch_selected

                # Add option to load specific list of samples
                if annotation_mode == "Batch Mode":
                    with st.expander("📋 Load Specific Sample List", expanded=False):
                        st.write(
                            "Paste a list of sample IDs (one per line or comma-separated):"
                        )
                        sample_list_text = st.text_area(
                            "Sample IDs",
                            height=200,
                            placeholder="bottleneck_021_01_ashley_szalwinski__cism__cisa__isa\nbottleneck_021_03_ashley_szalwinski__cism__cisa__isa\n...\n\nOr comma-separated:\nbottleneck_021_01_ashley_szalwinski__cism__cisa__isa, bottleneck_021_03_ashley_szalwinski__cism__cisa__isa, ...",
                            key="sample_list_input",
                        )

                        if st.button("📥 Load Sample List", type="secondary"):
                            if sample_list_text:
                                # Parse the input - handle both newline and comma separated
                                if "," in sample_list_text:
                                    # Comma-separated
                                    sample_ids = [
                                        s.strip()
                                        for s in sample_list_text.split(",")
                                        if s.strip()
                                    ]
                                else:
                                    # Newline-separated
                                    sample_ids = [
                                        s.strip()
                                        for s in sample_list_text.split("\n")
                                        if s.strip()
                                    ]

                                # Filter to only include samples that exist in the directory
                                valid_samples = []
                                invalid_samples = []
                                for sample_id in sample_ids:
                                    if sample_id in example_ids:
                                        valid_samples.append(sample_id)
                                    else:
                                        invalid_samples.append(sample_id)

                                if valid_samples:
                                    st.session_state.batch_selected = valid_samples
                                    st.success(
                                        f"✅ Loaded {len(valid_samples)} valid samples"
                                    )

                                    if invalid_samples:
                                        with st.expander(
                                            f"⚠️ {len(invalid_samples)} samples not found",
                                            expanded=False,
                                        ):
                                            for invalid in invalid_samples:
                                                st.write(f"- {invalid}")

                                    st.rerun()
                                else:
                                    st.error(
                                        "❌ None of the provided sample IDs were found in the selected directory"
                                    )
                            else:
                                st.warning("Please paste sample IDs to load")

                # Show currently selected samples in batch mode
                if annotation_mode == "Batch Mode" and selected_examples:
                    with st.expander(
                        f"📋 Selected Samples ({len(selected_examples)})",
                        expanded=False,
                    ):
                        # Display selected samples in a more compact format
                        for i, sample in enumerate(selected_examples):
                            if i < 10:  # Show first 10
                                st.write(f"{i + 1}. {sample}")
                            elif i == 10:
                                st.write(
                                    f"... and {len(selected_examples) - 10} more samples"
                                )
                                break

                # Navigation for batch mode
                if selected_examples and annotation_mode == "Batch Mode":
                    st.subheader("📖 Batch Navigation")

                    # Initialize current index
                    if "current_example_index" not in st.session_state:
                        st.session_state.current_example_index = 0

                    # Ensure index is within bounds
                    if st.session_state.current_example_index >= len(selected_examples):
                        st.session_state.current_example_index = 0

                    # Navigation controls
                    col1, col2, col3, col4 = st.columns([1, 1, 2, 1])

                    with col1:
                        if st.button(
                            "⬅️ Previous",
                            disabled=st.session_state.current_example_index <= 0,
                        ):
                            st.session_state.current_example_index -= 1
                            st.rerun()

                    with col2:
                        if st.button(
                            "➡️ Next",
                            disabled=st.session_state.current_example_index
                            >= len(selected_examples) - 1,
                        ):
                            st.session_state.current_example_index += 1
                            st.rerun()

                    with col3:
                        # Direct navigation
                        current_idx = st.selectbox(
                            "Jump to example:",
                            range(len(selected_examples)),
                            index=st.session_state.current_example_index,
                            format_func=lambda x: f"{x + 1}. {selected_examples[x]}",
                            key="example_navigator",
                        )
                        if current_idx != st.session_state.current_example_index:
                            st.session_state.current_example_index = current_idx
                            st.rerun()

                    with col4:
                        st.write(
                            f"**{st.session_state.current_example_index + 1}/{len(selected_examples)}**"
                        )

                    # Set current example for annotation
                    example_id = selected_examples[
                        st.session_state.current_example_index
                    ]
                elif selected_examples and annotation_mode == "Single Example":
                    # Single example mode
                    example_id = selected_examples[0]
                else:
                    example_id = None

                # Load and display the current example
                if example_id:
                    input_file = input_dir / f"{example_id}_input.json"
                    output_file = (
                        output_dir / f"{example_id}_output.json" if output_dir else None
                    )

                    if input_file.exists():
                        with open(input_file, "r") as f:
                            input_data = json.load(f)

                        output_data = None
                        if output_file and output_file.exists():
                            with open(output_file, "r") as f:
                                output_data = json.load(f)

                        st.success(f"Loaded example: {example_id}")

                        # Show progress for batch mode
                        if annotation_mode == "Batch Mode":
                            progress = (
                                st.session_state.current_example_index + 1
                            ) / len(selected_examples)
                            st.progress(progress)
                            st.info(
                                f"**Progress:** {st.session_state.current_example_index + 1}/{len(selected_examples)} - Current: {example_id}"
                            )
                    else:
                        st.error("Could not find input file")
                        input_data = None
                        output_data = None
                else:
                    st.warning("Please select an example to annotate")
                    input_data = None
                    output_data = None
            else:
                st.warning("No examples found in the selected input directory")
                input_data = None
                output_data = None
        else:
            st.info("👆 Please select an input directory to begin annotation")
            input_data = None
            output_data = None

    elif load_method == "Upload File":
        col1, col2 = st.columns(2)
        with col1:
            input_file = st.file_uploader("Upload input file", type="json")
            if input_file:
                try:
                    input_data = json.load(input_file)
                    example_id = input_file.name.replace("_input.json", "").replace(
                        ".json", ""
                    )
                except Exception as e:
                    st.error(f"Error loading input file: {e}")

        with col2:
            output_file = st.file_uploader(
                "Upload output file (for scoring)", type="json"
            )
            if output_file:
                try:
                    output_data = json.load(output_file)
                except Exception as e:
                    st.error(f"Error loading output file: {e}")

    elif load_method == "From Generated":
        if st.session_state.generated_examples:
            selected_id = st.selectbox(
                "Select Generated Example",
                list(st.session_state.generated_examples.keys()),
            )
            example_data = st.session_state.generated_examples[selected_id]
            example_id = selected_id

            # Split into input/output
            input_data, output_data = prepare_single_example(example_data)
        else:
            st.warning("No generated examples available")

    # Annotation interface
    if input_data and example_id:
        st.subheader("Example Annotation")

        # Create annotation directory if it doesn't exist
        annotation_dir = Path("annotations")
        annotation_dir.mkdir(exist_ok=True)

        # Determine batch mode status
        is_batch_mode = (
            hasattr(st.session_state, "selected_input_dir")
            and st.session_state.selected_input_dir
            and hasattr(st.session_state, "current_example_index")
        )

        # Get selected examples for batch mode
        selected_examples = getattr(st.session_state, "batch_selected", [])
        if not selected_examples and is_batch_mode:
            # Fallback to single example if batch_selected is empty
            selected_examples = [example_id]

        # Display complete world model
        with st.expander("🌍 Complete World Model", expanded=True):
            world_model = input_data.get("world_model", {})

            # Persona ID
            if "persona_id" in world_model:
                st.write(f"**Persona ID:** `{world_model['persona_id']}`")

            # Persona Information (if embedded in world model)
            if "persona" in world_model:
                st.write("### 👤 Persona Information")
                persona = world_model.get("persona", {})

                col1, col2 = st.columns(2)
                with col1:
                    if isinstance(persona, dict):
                        st.write(f"**Name:** {persona.get('name', 'Unknown')}")
                        st.write(
                            f"**Occupation:** {persona.get('occupation', 'Unknown')}"
                        )
                        st.write(
                            f"**Location:** {persona.get('location', 'Not specified')}"
                        )

                with col2:
                    if isinstance(persona, dict):
                        about = persona.get("about", "")
                        if about:
                            st.write("**About:**")
                            st.write(about[:200] + "..." if len(about) > 200 else about)

            # Organizational Structure
            if "organizational_structure" in world_model:
                st.write("### 🏢 Organizational Structure")
                org_structure = world_model.get("organizational_structure", {})

                col1, col2 = st.columns(2)
                with col1:
                    st.write(
                        f"**Company:** {org_structure.get('company_name', 'Unknown')}"
                    )
                    st.write(
                        f"**Department:** {org_structure.get('department', 'Not specified')}"
                    )
                    st.write(
                        f"**Team Size:** {org_structure.get('team_size', 'Not specified')}"
                    )
                    st.write(
                        f"**Reporting To:** {org_structure.get('reporting_to', 'Not specified')}"
                    )

                with col2:
                    # Industry and hierarchy info if available
                    if "industry" in org_structure:
                        st.write(f"**Industry:** {org_structure.get('industry')}")
                    if "company_size" in org_structure:
                        st.write(
                            f"**Company Size:** {org_structure.get('company_size')}"
                        )
                    if "hierarchy_level" in org_structure:
                        st.write(
                            f"**Hierarchy Level:** {org_structure.get('hierarchy_level')}"
                        )

                # Key meetings
                if "key_meetings" in org_structure:
                    st.write("**Key Meetings:**")
                    meetings = org_structure.get("key_meetings", [])
                    if meetings:
                        cols = st.columns(3)
                        for idx, meeting in enumerate(meetings):
                            cols[idx % 3].write(f"• {meeting}")

            # Relationships
            relationships = world_model.get("relationships", [])
            if relationships:
                st.write("### 👥 Key Relationships")

                # Group relationships by type
                rel_by_type = {}
                for rel in relationships:
                    rel_type = rel.get("type", rel.get("relationship_type", "other"))
                    if rel_type not in rel_by_type:
                        rel_by_type[rel_type] = []
                    rel_by_type[rel_type].append(rel)

                # Display relationships by type
                for rel_type, rels in rel_by_type.items():
                    st.write(f"**{rel_type.replace('_', ' ').title()}s:**")
                    for rel in rels:
                        name = rel.get("name", "Unknown")
                        dept = rel.get("department", "")
                        context = rel.get(
                            "relationship_context", rel.get("description", "")
                        )

                        dept_str = f" ({dept})" if dept else ""
                        st.write(f"• **{name}{dept_str}**: {context}")

            # Personal Context
            personal_context = world_model.get("personal_context", {})
            if personal_context:
                st.write("### 🎯 Personal Context")

                # Work style
                if "work_style" in personal_context:
                    st.write("**Work Style:**")
                    st.info(personal_context.get("work_style"))

                # Communication preferences
                if "communication_preferences" in personal_context:
                    st.write("**Communication Preferences:**")
                    prefs = personal_context.get("communication_preferences", [])
                    if prefs:
                        for pref in prefs:
                            st.write(f"• {pref}")

                # Current priorities
                if "current_priorities" in personal_context:
                    st.write("**Current Priorities:**")
                    priorities = personal_context.get("current_priorities", [])
                    if priorities:
                        for priority in priorities:
                            st.write(f"• {priority}")

                # Pain points
                if "pain_points" in personal_context:
                    st.write("**Pain Points:**")
                    pain_points = personal_context.get("pain_points", [])
                    if pain_points:
                        for pain_point in pain_points:
                            st.write(f"• {pain_point}")

                # Goals
                if "goals" in personal_context:
                    st.write("**Goals:**")
                    goals = personal_context.get("goals", [])
                    if goals:
                        for goal in goals[:5]:  # Show first 5
                            st.write(f"• {goal}")
                        if len(goals) > 5:
                            st.write(f"*... and {len(goals) - 5} more goals*")

                # Challenges
                if "challenges" in personal_context:
                    st.write("**Challenges:**")
                    challenges = personal_context.get("challenges", [])
                    if challenges:
                        for challenge in challenges[:5]:  # Show first 5
                            st.write(f"• {challenge}")
                        if len(challenges) > 5:
                            st.write(f"*... and {len(challenges) - 5} more challenges*")

                # Preferences (if not communication preferences)
                if "preferences" in personal_context and isinstance(
                    personal_context["preferences"], dict
                ):
                    st.write("**Other Preferences:**")
                    prefs = personal_context.get("preferences", {})
                    for key, value in prefs.items():
                        st.write(f"• **{key.replace('_', ' ').title()}**: {value}")

            # Recurring Events
            if "recurring_events" in world_model:
                st.write("### 📅 Recurring Events")
                events = world_model.get("recurring_events", [])
                if events:
                    for event in events[:5]:  # Show first 5
                        if isinstance(event, dict):
                            st.write(
                                f"• **{event.get('name', 'Unknown')}**: {event.get('frequency', '')} - {event.get('description', '')}"
                            )
                        else:
                            st.write(f"• {event}")
                    if len(events) > 5:
                        st.write(f"*... and {len(events) - 5} more recurring events*")

            # Task Patterns
            if "task_patterns" in world_model:
                st.write("### 📊 Task Patterns")
                patterns = world_model.get("task_patterns", [])
                if patterns:
                    for pattern in patterns[:5]:  # Show first 5
                        if isinstance(pattern, dict):
                            st.write(
                                f"• **{pattern.get('name', 'Unknown')}**: {pattern.get('description', '')}"
                            )
                        else:
                            st.write(f"• {pattern}")
                    if len(patterns) > 5:
                        st.write(f"*... and {len(patterns) - 5} more task patterns*")

            # Context Difficulty
            if "context_difficulty" in world_model:
                difficulty = world_model.get("context_difficulty", "unknown")
                st.write(f"### 🎚️ Context Difficulty: **{str(difficulty).upper()}**")

            # Available Actions - Detailed View
            st.write("### 🎬 Available Actions")
            actions = world_model.get("available_actions", [])
            if actions:
                st.info(f"Total available actions: {len(actions)}")

                # Group actions by type
                actions_by_type = {}
                for action in actions:
                    action_type = action.get("type", "other")
                    if action_type not in actions_by_type:
                        actions_by_type[action_type] = []
                    actions_by_type[action_type].append(action)

                # Display actions grouped by type
                for action_type, type_actions in actions_by_type.items():
                    st.write(
                        f"**{action_type.replace('_', ' ').title()} Actions ({len(type_actions)}):**"
                    )

                    for i, action in enumerate(type_actions):
                        action_id = action.get("id", "Unknown")
                        action_desc = action.get("description", "No description")
                        action_name = (
                            action_id.replace("_", " ").title()
                            if action_id != "Unknown"
                            else "Unknown Action"
                        )

                        # Use container instead of nested expander
                        with st.container():
                            st.write(f"**{i + 1}. {action_name}**")
                            col1, col2 = st.columns([1, 2])
                            with col1:
                                st.write(f"**ID:** `{action_id}`")
                                st.write(f"**Type:** {action_type}")
                            with col2:
                                st.write(f"**Description:** {action_desc}")

                            # Show parameters schema
                            params_schema = action.get("params_schema", {})
                            if params_schema:
                                required_params = params_schema.get("required", [])
                                if required_params:
                                    st.write(
                                        f"**Required Parameters:** {', '.join(f'`{p}`' for p in required_params)}"
                                    )

                                # Show constraints if any
                                constraints = action.get("constraints", [])
                                if constraints:
                                    st.write(
                                        f"**Constraints:** {'; '.join(constraints)}"
                                    )

                            st.write("---")
            else:
                st.warning("No actions available in the world model")

        # Display data points
        st.subheader("Available Data Points")

        data_points = input_data.get("data_points", [])

        # Create a mapping from item ID to display index
        item_id_to_index = {item.get("id"): idx for idx, item in enumerate(data_points)}

        # Display items
        st.write(f"Total items: {len(data_points)}")

        with st.expander("Data Points", expanded=True):
            for idx, item in enumerate(data_points):
                item_id = item.get("id", f"item_{idx}")
                item_type = item.get("type", "Unknown")
                content = item.get("payload", item.get("content", {}))

                # Create item header with emoji based on type
                type_emoji = {
                    "email": "📧",
                    "event": "📅",
                    "document": "📄",
                    "calendar": "📅",
                    "message": "💬",
                    "task": "✅",
                    "note": "📝",
                }.get(item_type.lower(), "📋")

                # Create a collapsible expander for each data point
                # Get a preview for the expander title
                preview = ""
                if isinstance(content, dict):
                    if item_type.lower() == "email":
                        subject = content.get("subject", "No subject")
                        preview = (
                            f" - {subject[:50]}..."
                            if len(subject) > 50
                            else f" - {subject}"
                        )
                    elif item_type.lower() in ["event", "calendar"]:
                        title = content.get("title", content.get("name", "Unknown"))
                        preview = (
                            f" - {title[:50]}..." if len(title) > 50 else f" - {title}"
                        )
                    elif item_type.lower() == "document":
                        title = content.get("title", content.get("name", "Unknown"))
                        preview = (
                            f" - {title[:50]}..." if len(title) > 50 else f" - {title}"
                        )

                with st.expander(
                    f"{type_emoji} [{idx}] {item_type.upper()} - ID: `{item_id}`{preview}",
                    expanded=False,
                ):
                    # Display content based on type
                    if isinstance(content, dict):
                        # Email content
                        if item_type.lower() == "email":
                            col1, col2 = st.columns([3, 1])
                            with col1:
                                st.write(
                                    f"**From:** {content.get('sender', 'Unknown')}"
                                )
                                # Handle both 'to' and 'recipients' fields
                                recipients = content.get(
                                    "to", content.get("recipients", "Unknown")
                                )
                                if isinstance(recipients, list):
                                    recipients = (
                                        ", ".join(recipients)
                                        if recipients
                                        else "Unknown"
                                    )
                                st.write(f"**To:** {recipients}")
                                st.write(
                                    f"**Subject:** {content.get('subject', 'No subject')}"
                                )
                            with col2:
                                st.write(
                                    f"**Date:** {content.get('date', content.get('timestamp', 'Unknown'))}"
                                )
                                if content.get("priority"):
                                    st.write(f"**Priority:** {content.get('priority')}")

                            # Email body
                            body = content.get("body", content.get("content", ""))
                            if body:
                                st.write("**Content:**")
                                # Use a styled container for better readability
                                with st.container():
                                    st.text_area(
                                        "Email content",
                                        body,
                                        height=150,
                                        disabled=True,
                                        key=f"email_body_{idx}",
                                        label_visibility="collapsed",
                                    )

                        # Calendar event content
                        elif item_type.lower() in ["event", "calendar"]:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(
                                    f"**Title:** {content.get('title', content.get('name', 'Unknown'))}"
                                )
                                st.write(
                                    f"**Date:** {content.get('date', content.get('start_time', 'Unknown'))}"
                                )
                                # Handle end time if available
                                end_time = content.get("end_time")
                                if end_time:
                                    st.write(f"**End Time:** {end_time}")
                                duration = content.get("duration", "Not specified")
                                if duration != "Not specified":
                                    st.write(f"**Duration:** {duration}")
                            with col2:
                                st.write(
                                    f"**Location:** {content.get('location', 'Not specified')}"
                                )
                                st.write(
                                    f"**Organizer:** {content.get('organizer', 'Unknown')}"
                                )

                            # Description
                            description = content.get(
                                "description", content.get("agenda", "")
                            )
                            if description:
                                st.write("**Description:**")
                                st.text_area(
                                    "Event description",
                                    description,
                                    height=100,
                                    disabled=True,
                                    key=f"event_desc_{idx}",
                                    label_visibility="collapsed",
                                )

                            # Attendees
                            attendees = content.get(
                                "attendees", content.get("participants", [])
                            )
                            if attendees:
                                if isinstance(attendees, list):
                                    st.write(f"**Attendees:** {', '.join(attendees)}")
                                else:
                                    st.write(f"**Attendees:** {attendees}")

                        # Document content
                        elif item_type.lower() == "document":
                            st.write(
                                f"**Title:** {content.get('title', content.get('name', 'Unknown'))}"
                            )
                            st.write(
                                f"**Author:** {content.get('author', content.get('created_by', 'Unknown'))}"
                            )
                            doc_type = content.get(
                                "doc_type",
                                content.get("format", content.get("type", "Unknown")),
                            )
                            st.write(f"**Type:** {doc_type}")

                            # Document content/summary
                            doc_content = content.get(
                                "content",
                                content.get(
                                    "summary",
                                    content.get("text", content.get("body", "")),
                                ),
                            )
                            if doc_content:
                                st.write("**Content:**")
                                st.text_area(
                                    "Document content",
                                    doc_content,
                                    height=200,
                                    disabled=True,
                                    key=f"doc_content_{idx}",
                                    label_visibility="collapsed",
                                )

                        # Generic content display for other types
                        else:
                            # Display all fields in a structured way
                            for key, value in content.items():
                                if key in ["id", "type"]:  # Skip redundant fields
                                    continue

                                # Format key nicely
                                formatted_key = key.replace("_", " ").title()

                                if isinstance(value, (list, tuple)):
                                    if value:  # Only show non-empty lists
                                        st.write(
                                            f"**{formatted_key}:** {', '.join(str(v) for v in value)}"
                                        )
                                elif isinstance(value, dict):
                                    st.write(f"**{formatted_key}:**")
                                    st.json(value)
                                elif isinstance(value, str) and len(value) > 100:
                                    st.write(f"**{formatted_key}:**")
                                    st.text_area(
                                        f"{formatted_key} content",
                                        value,
                                        height=100,
                                        disabled=True,
                                        key=f"generic_{idx}_{key}",
                                        label_visibility="collapsed",
                                    )
                                elif value is not None and str(value).strip():
                                    st.write(f"**{formatted_key}:** {value}")

                st.write("---")

        # Annotation inputs
        st.subheader("Your Annotations")

        # Document selection by ID - using text area for copy-paste
        st.write("**Enter document IDs relevant to the bottleneck:**")
        doc_ids_text = st.text_area(
            "Document IDs (one per line or comma-separated)",
            height=100,
            placeholder="doc_123\ndoc_456\ndoc_789\n\nOr comma-separated:\ndoc_123, doc_456, doc_789",
            help="Paste the IDs of data points you believe are relevant to addressing the bottleneck. You can copy IDs from the data points above.",
            key="doc_ids_input",
        )

        # Parse the input to get selected IDs
        selected_doc_ids = []
        if doc_ids_text:
            # Check if comma-separated or newline-separated
            if "," in doc_ids_text:
                # Comma-separated
                selected_doc_ids = [
                    id.strip() for id in doc_ids_text.split(",") if id.strip()
                ]
            else:
                # Newline-separated
                selected_doc_ids = [
                    id.strip() for id in doc_ids_text.split("\n") if id.strip()
                ]

        # Show the parsed IDs for confirmation
        if selected_doc_ids:
            st.info(
                f"Selected {len(selected_doc_ids)} document IDs: {', '.join(selected_doc_ids[:5])}"
                + (
                    f"... and {len(selected_doc_ids) - 5} more"
                    if len(selected_doc_ids) > 5
                    else ""
                )
            )

        # Bottleneck identification
        identified_bottleneck = st.text_area(
            "Describe the bottleneck you identified:",
            help="Write a natural language description explaining the concerning pattern/issue and why intervention is needed",
        )

        # Action selection
        st.write("**Select Action to Execute:**")
        actions = world_model.get("available_actions", [])

        # Initialize variables with defaults
        params_json = "{}"
        selected_action_idx = 0

        if actions:
            # Create action descriptions for dropdown
            action_descriptions = []
            for i, action in enumerate(actions):
                if isinstance(action, dict):
                    action_id = action.get("id", "Unknown")
                    action_type = action.get("type", "Unknown")
                    desc = action.get("description", "")
                    # Create readable name from ID
                    action_name = (
                        action_id.replace("_", " ").title()
                        if action_id != "Unknown"
                        else "Unknown Action"
                    )
                    desc_preview = desc[:50] + "..." if len(desc) > 50 else desc
                    action_descriptions.append(
                        f"{i + 1}. {action_name} ({action_type}) - {desc_preview}"
                    )

            selected_action_idx = st.selectbox(
                "Choose an action:",
                options=list(range(len(actions))),
                format_func=lambda x: (
                    action_descriptions[x]
                    if x < len(action_descriptions)
                    else "Unknown"
                ),
            )

            # Display selected action details
            if selected_action_idx < len(actions):
                selected_action = actions[selected_action_idx]

                with st.expander("Selected Action Details", expanded=True):
                    if isinstance(selected_action, dict):
                        st.write(
                            f"**Action ID:** `{selected_action.get('id', 'unknown')}`"
                        )
                        action_name = (
                            selected_action.get("id", "unknown")
                            .replace("_", " ")
                            .title()
                        )
                        st.write(f"**Name:** {action_name}")
                        st.write(f"**Type:** {selected_action.get('type', 'Unknown')}")
                        st.write(
                            f"**Description:** {selected_action.get('description', 'No description')}"
                        )

                        # Show parameters schema
                        params_schema = selected_action.get("params_schema", {})
                        if params_schema:
                            st.write("**Required Parameters:**")
                            required_params = params_schema.get("required", [])
                            param_template = {}

                            if required_params:
                                for param_name in required_params:
                                    st.write(f"- `{param_name}`")
                                    # Build template based on common parameter names
                                    if "email" in param_name.lower():
                                        param_template[param_name] = "user@example.com"
                                    elif "subject" in param_name.lower():
                                        param_template[param_name] = "Subject line here"
                                    elif (
                                        "message" in param_name.lower()
                                        or "body" in param_name.lower()
                                    ):
                                        param_template[param_name] = (
                                            "Message content here"
                                        )
                                    elif (
                                        "time" in param_name.lower()
                                        or "date" in param_name.lower()
                                    ):
                                        param_template[param_name] = (
                                            "2024-10-20T10:00:00Z"
                                        )
                                    elif "priority" in param_name.lower():
                                        param_template[param_name] = "high"
                                    elif (
                                        "participants" in param_name.lower()
                                        or "attendees" in param_name.lower()
                                    ):
                                        param_template[param_name] = [
                                            "person1@example.com",
                                            "person2@example.com",
                                        ]
                                    elif "agenda" in param_name.lower():
                                        param_template[param_name] = (
                                            "Meeting agenda here"
                                        )
                                    elif "task" in param_name.lower():
                                        param_template[param_name] = "Task description"
                                    elif "assignee" in param_name.lower():
                                        param_template[param_name] = (
                                            "assignee@example.com"
                                        )
                                    elif "deadline" in param_name.lower():
                                        param_template[param_name] = "2024-10-25"
                                    elif "channel" in param_name.lower():
                                        param_template[param_name] = "#general"
                                    elif "document_id" in param_name.lower():
                                        param_template[param_name] = "doc_123"
                                    elif "meeting_id" in param_name.lower():
                                        param_template[param_name] = "meeting_456"
                                    else:
                                        param_template[param_name] = ""

                            # Show constraints if any
                            constraints = selected_action.get("constraints", [])
                            if constraints:
                                st.write("**Constraints:**")
                                for constraint in constraints:
                                    st.write(f"- {constraint}")

                            if param_template:
                                st.write("**Parameter Template:**")
                                st.code(
                                    json.dumps(param_template, indent=2),
                                    language="json",
                                )

            # Action parameters input
            st.write("**Action Parameters (JSON):**")
            params_json = st.text_area(
                "Enter parameters for the selected action:",
                value="{}",
                height=150,
                help="Enter parameters for the selected action as JSON. Use the template above as a guide.",
            )
        else:
            st.warning("No actions available. Please check the world model.")
            st.write("**Action Parameters (JSON):**")
            params_json = st.text_area(
                "Enter parameters for the selected action:",
                value="{}",
                height=150,
                help="No actions available to configure.",
                disabled=True,
            )

        # Annotator name
        annotator_name = st.text_input(
            "Annotator Name", "Anonymous", help="Enter your name for tracking purposes"
        )

        # Save annotation
        col1, col2 = st.columns(2)

        with col1:
            # Choose button label based on mode
            if is_batch_mode and len(selected_examples) > 1:
                is_last_example = (
                    hasattr(st.session_state, "current_example_index")
                    and st.session_state.current_example_index
                    >= len(selected_examples) - 1
                )
                button_label = (
                    "💾 Save & Finish" if is_last_example else "💾 Save & Next"
                )
            else:
                button_label = "💾 Save Annotation"

            if st.button(button_label, type="primary"):
                try:
                    parameters = json.loads(params_json)

                    selected_action = (
                        actions[selected_action_idx]
                        if selected_action_idx < len(actions)
                        else None
                    )
                    if isinstance(selected_action, dict):
                        action_id = selected_action.get("id", "unknown")
                    else:
                        action_id = "unknown"

                    # Create annotation in new format
                    annotation = Annotation(
                        retrieved_document_ids=selected_doc_ids,
                        bottleneck_description=identified_bottleneck,
                        action_selection=ActionSelection(
                            name=action_id, schema=parameters
                        ),
                    )

                    # Validate annotation
                    errors = annotation.validate()
                    if errors:
                        for error in errors:
                            st.error(f"❌ {error}")
                    else:
                        # Save annotation
                        metadata = {
                            "annotator": annotator_name,
                            "batch_mode": is_batch_mode,
                            "world_model_summary": {
                                "persona_name": world_model.get("persona", {}).get(
                                    "name", "Unknown"
                                ),
                                "persona_id": world_model.get("persona_id", "Unknown"),
                                "total_data_points": len(data_points),
                                "available_actions": len(actions),
                                "context_difficulty": world_model.get(
                                    "context_difficulty", "unknown"
                                ),
                            },
                        }

                        filepath = save_annotation(
                            annotation,
                            example_id,
                            annotation_dir,
                            annotation_type="annotation",
                            metadata=metadata,
                        )

                        st.success(f"✅ Annotation saved to: {filepath.name}")

                        # Handle batch mode navigation
                        if is_batch_mode:
                            if not is_last_example:
                                # Auto-advance to next example
                                st.session_state.current_example_index += 1
                                st.rerun()
                            else:
                                st.success("🎉 All selected examples completed!")
                                st.balloons()
                        else:
                            # Single mode - offer download
                            with open(filepath, "r") as f:
                                annotation_json = f.read()

                            st.download_button(
                                "📥 Download Annotation",
                                data=annotation_json,
                                file_name=filepath.name,
                                mime="application/json",
                            )

                except json.JSONDecodeError:
                    st.error("❌ Invalid JSON in parameters")
                except Exception as e:
                    st.error(f"❌ Error saving annotation: {e}")

        with col2:
            # Add skip button for batch mode
            if is_batch_mode and len(selected_examples) > 1:
                is_last_example = (
                    hasattr(st.session_state, "current_example_index")
                    and st.session_state.current_example_index
                    >= len(selected_examples) - 1
                )
                if st.button("⏭️ Skip Example"):
                    if not is_last_example:
                        st.session_state.current_example_index += 1
                        st.rerun()
                    else:
                        st.info("This is the last example in your selection")

            # Show current annotation status
            if output_data:
                st.write("**Ground Truth Available:** ✅")
                st.write("Load output file to enable scoring")
            else:
                st.write("**Ground Truth Available:** ❌")
                st.write("Load output file to enable scoring")
                st.write("Load output file to enable scoring")

elif page == "Batch Scoring":
    st.title("Batch Scoring Dashboard")

    st.info(
        "Use the 'Score Annotations' page with 'Score Batch from Directories' option for batch scoring."
    )

elif page == "Score Annotations":
    st.title("Score Annotations")

    # Choose scoring method
    scoring_method = st.radio(
        "Scoring Method",
        ["Score Single Files", "Score Batch Directories"],
        index=1,  # Default to batch scoring
    )

    if scoring_method == "Score Single Files":
        # Single file scoring interface
        st.subheader("Score Single Annotation")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.write("**Prediction File**")
            prediction_file = st.file_uploader(
                "Upload prediction/annotation file", type="json", key="single_pred"
            )
            if prediction_file:
                try:
                    pred_data = json.load(prediction_file)

                    # Handle different prediction formats
                    if "retrieved_document_ids" in pred_data:  # Standard format
                        annotation = Annotation.from_dict(pred_data)
                    elif "retrieved_documents" in pred_data:  # baseline_agent format
                        action_data = pred_data.get("action", {})
                        annotation = Annotation(
                            retrieved_document_ids=pred_data.get(
                                "retrieved_documents", []
                            ),
                            bottleneck_description=pred_data.get("bottleneck", ""),
                            action_selection=ActionSelection(
                                name=action_data.get("function_name", ""),
                                schema=action_data.get("parameters", {}),
                            ),
                        )
                    else:
                        # Try other formats
                        annotation = Annotation(
                            retrieved_document_ids=pred_data.get(
                                "selected_ids", pred_data.get("retrieved_ids", [])
                            ),
                            bottleneck_description=pred_data.get(
                                "bottleneck_description",
                                pred_data.get("bottleneck", ""),
                            ),
                            action_selection=ActionSelection(
                                name=pred_data.get("action", {}).get(
                                    "name",
                                    pred_data.get("action", {}).get(
                                        "function_name", ""
                                    ),
                                ),
                                schema=pred_data.get("action", {}).get(
                                    "parameters",
                                    pred_data.get("action", {}).get("schema", {}),
                                ),
                            ),
                        )

                    st.success("✓ Valid prediction format")
                except Exception as e:
                    st.error(f"Invalid format: {e}")
                    annotation = None

        with col2:
            st.write("**Label File (Ground Truth)**")
            label_file = st.file_uploader(
                "Upload label/output file", type="json", key="single_label"
            )
            if label_file:
                try:
                    output_data = json.load(label_file)
                    st.success("✓ Loaded ground truth")
                except Exception as e:
                    st.error(f"Error loading: {e}")
                    output_data = None

        with col3:
            st.write("**Input File (Optional)**")
            input_file = st.file_uploader(
                "Upload input file for context", type="json", key="single_input"
            )
            if input_file:
                try:
                    input_data = json.load(input_file)
                    st.success("✓ Loaded input context")
                except Exception as e:
                    st.error(f"Error loading: {e}")
                    input_data = None
            else:
                input_data = None

        # Score button
        if st.button(
            "🎯 Score Files", disabled=(not prediction_file or not label_file)
        ):
            if (
                "annotation" in locals()
                and annotation
                and "output_data" in locals()
                and output_data
            ):
                # Initialize scorer
                llm_func = None
                try:
                    client = get_openai_client()

                    def llm_func(prompt: str) -> str:
                        response = client.chat.completions.create(
                            model="gpt-4o-mini",
                            messages=[
                                {
                                    "role": "system",
                                    "content": "You are a helpful assistant that evaluates agent performance.",
                                },
                                {"role": "user", "content": prompt},
                            ],
                            temperature=0,
                            response_format={"type": "json_object"},
                        )
                        return response.choices[0].message.content

                except Exception as e:
                    st.error(f"Error using LLM: {e}")
                    st.info("Using exact match scoring (LLM unavailable)")

                    scorer = ChecklistScorer(llm_generate_func=llm_func)

                # Score
                try:
                    scoring_result = scorer.score_annotation(
                        annotation, output_data, input_data
                    )

                    # Display results
                    st.success("✅ Scoring complete!")

                    # Overall score with color
                    overall = scoring_result.overall_score
                    color = (
                        "green"
                        if overall >= 0.7
                        else "orange" if overall >= 0.4 else "red"
                    )
                    st.markdown(
                        f"### Overall Score: <span style='color:{color}; font-size:32px'>{overall:.2%}</span>",
                        unsafe_allow_html=True,
                    )

                    # Component scores - Retrieval metrics prominently displayed
                    st.markdown("### 🔍 Retrieval Metrics")
                    col1, col2, col3 = st.columns(3)

                    # Extract retrieval details for display
                    retrieval_details = scoring_result.retrieval_details
                    if isinstance(retrieval_details, dict):
                        precision = retrieval_details.get("precision", 0.0)
                        recall = retrieval_details.get("recall", 0.0)
                        f1_score = retrieval_details.get("f1_score", 0.0)
                    else:
                        precision = recall = f1_score = 0.0

                    with col1:
                        # F1 Score (main retrieval metric)
                        f1_color = (
                            "green"
                            if f1_score >= 0.7
                            else "orange" if f1_score >= 0.4 else "red"
                        )
                        st.markdown("**F1 Score**")
                        st.markdown(
                            f"<span style='color:{f1_color}; font-size:24px; font-weight:bold'>{f1_score:.2%}</span>",
                            unsafe_allow_html=True,
                        )

                    with col2:
                        # Precision
                        prec_color = (
                            "green"
                            if precision >= 0.7
                            else "orange" if precision >= 0.4 else "red"
                        )
                        st.markdown("**Precision**")
                        st.markdown(
                            f"<span style='color:{prec_color}; font-size:24px; font-weight:bold'>{precision:.2%}</span>",
                            unsafe_allow_html=True,
                        )

                    with col3:
                        # Recall
                        recall_color = (
                            "green"
                            if recall >= 0.7
                            else "orange" if recall >= 0.4 else "red"
                        )
                        st.markdown("**Recall**")
                        st.markdown(
                            f"<span style='color:{recall_color}; font-size:24px; font-weight:bold'>{recall:.2%}</span>",
                            unsafe_allow_html=True,
                        )

                    # Other component scores
                    st.markdown("### 🎯 Other Components")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric(
                            "🎯 Identification",
                            f"{scoring_result.identification_score:.2%}",
                        )
                    with col2:
                        st.metric(
                            "✅ Task Selection",
                            f"{scoring_result.task_selection_score:.2%}",
                        )

                    # Detailed breakdown
                    with st.expander("📊 Detailed Scoring Breakdown", expanded=False):
                        # Create tabs for different aspects
                        tab1, tab2, tab3 = st.tabs(
                            ["Retrieval", "Identification", "Task Selection"]
                        )

                        with tab1:
                            details = scoring_result.retrieval_details
                            if isinstance(details, dict):
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.write("**Metrics:**")
                                    st.write(
                                        f"- Precision: {details.get('precision', 0):.2%}"
                                    )
                                    st.write(
                                        f"- Recall: {details.get('recall', 0):.2%}"
                                    )
                                    st.write(
                                        f"- F1 Score: {details.get('f1_score', 0):.2%}"
                                    )
                                with col2:
                                    st.write("**Counts:**")
                                    st.write(
                                        f"- True Positives: {details.get('true_positives', 0)}"
                                    )
                                    st.write(
                                        f"- False Positives: {details.get('false_positives', 0)}"
                                    )
                                    st.write(
                                        f"- False Negatives: {details.get('false_negatives', 0)}"
                                    )

                        with tab2:
                            details = scoring_result.identification_details
                            if isinstance(details, dict):
                                st.write(
                                    f"**Method:** {details.get('method', 'unknown')}"
                                )
                                if "identified" in details:
                                    st.write("**Identified:**")
                                    st.info(details["identified"])
                                if "expected" in details:
                                    st.write("**Expected:**")
                                    st.success(details["expected"])
                                if "reasoning" in details:
                                    st.write("**LLM Reasoning:**")
                                    st.write(details["reasoning"])

                        with tab3:
                            details = scoring_result.task_selection_details
                            if isinstance(details, dict):
                                st.write(
                                    f"**Action Match:** {'✅' if details.get('action_correct') else '❌'}"
                                )
                                st.write(
                                    f"**Selected:** `{details.get('selected_action', 'none')}`"
                                )
                                st.write(
                                    f"**Expected:** `{details.get('expected_action', 'none')}`"
                                )
                                if "parameter_score" in details:
                                    st.write(
                                        f"**Parameter Score:** {details['parameter_score']:.2%}"
                                    )

                    # Download results
                    result_json = json.dumps(scoring_result.to_dict(), indent=2)
                    st.download_button(
                        "💾 Download Scoring Results",
                        data=result_json,
                        file_name=f"scoring_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                    )

                except Exception as e:
                    st.error(f"Scoring failed: {e}")
                    import traceback

                    st.error(traceback.format_exc())

    else:  # Score Batch Directories
        st.subheader("Score Batch from Directories")

        # Helper function to list JSON files in a directory
        def list_json_files(directory_path: str) -> List[str]:
            """List all JSON files in a directory."""
            try:
                path = Path(directory_path).expanduser()
                if path.exists() and path.is_dir():
                    files = sorted([f.name for f in path.glob("*.json")])
                    return files
                return []
            except Exception:
                return []

        # Helper function to extract example ID from filename
        def extract_example_id(filename: str) -> str:
            """Extract example ID from various filename formats."""
            # Remove .json extension
            name = filename.replace(".json", "")

            # Handle various formats:
            # - example_id_annotation.json -> example_id
            # - example_id_annotation_20250920_094727.json -> example_id
            # - example_id_output.json -> example_id
            # - example_id.json -> example_id
            # - bottleneck_001_01_name.json -> bottleneck_001_01_name

            # Handle annotation files with timestamps first (more specific pattern)
            import re

            annotation_match = re.search(r"^(.+)_annotation_\d{8}_\d{6}$", name)
            if annotation_match:
                return annotation_match.group(1)

            # Remove common suffixes
            for suffix in [
                "_annotation",
                "_output",
                "_input",
                "_prediction",
                "_pred",
                "_label",
                "_results",
            ]:
                if name.endswith(suffix):
                    name = name[: -len(suffix)]
                    break

            return name

        # Directory selection
        col1, col2 = st.columns(2)

        with col1:
            st.write("### 📁 Predictions Directory")
            st.write("Directory containing model predictions in annotation format")

            # Text input with browse hint
            pred_dir = st.text_input(
                "Predictions directory path",
                value="",
                placeholder="e.g., ~/Downloads/baseline_gpt_5",
                help="Enter the full path to your predictions directory",
            )

            # Show directory contents if valid
            if pred_dir:
                pred_files = list_json_files(pred_dir)
                if pred_files:
                    st.success(f"✓ Found {len(pred_files)} prediction files")
                    with st.expander("Preview files", expanded=False):
                        for f in pred_files[:10]:  # Show first 10
                            st.write(f"- {f}")
                        if len(pred_files) > 10:
                            st.write(f"... and {len(pred_files) - 10} more")
                else:
                    st.warning("No JSON files found in directory")

        with col2:
            st.write("### 📁 Labels Directory")
            st.write("Directory containing ground truth in output format")

            # Text input with browse hint
            label_dir = st.text_input(
                "Labels directory path",
                value="",
                placeholder="e.g., batch_examples/outputs/batch_name",
                help="Enter the full path to your labels directory",
            )

            # Show directory contents if valid
            if label_dir:
                label_files = list_json_files(label_dir)
                if label_files:
                    st.success(f"✓ Found {len(label_files)} label files")
                    with st.expander("Preview files", expanded=False):
                        for f in label_files[:10]:  # Show first 10
                            st.write(f"- {f}")
                        if len(label_files) > 10:
                            st.write(f"... and {len(label_files) - 10} more")
                else:
                    st.warning("No JSON files found in directory")

        # Optional input directory
        st.write("### 📁 Input Directory (Optional)")
        input_dir = st.text_input(
            "Input directory path",
            value="",
            placeholder="e.g., batch_examples/inputs/batch_name",
            help="Optional: Directory containing input files for additional context",
        )

        # File matching preview
        if pred_dir and label_dir and pred_files and label_files:
            st.write("### 🔗 File Matching Preview")

            # Extract IDs and match files
            pred_ids = {extract_example_id(f): f for f in pred_files}
            label_ids = {extract_example_id(f): f for f in label_files}

            # Find matches
            matched_ids = set(pred_ids.keys()) & set(label_ids.keys())
            pred_only = set(pred_ids.keys()) - set(label_ids.keys())
            label_only = set(label_ids.keys()) - set(pred_ids.keys())

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Matched Examples", len(matched_ids))
            with col2:
                st.metric("Predictions Only", len(pred_only))
            with col3:
                st.metric("Labels Only", len(label_only))

            # Show details
            with st.expander("Matching Details", expanded=False):
                if matched_ids:
                    st.write("**Matched examples:**")
                    for id in sorted(list(matched_ids))[:5]:
                        st.write(f"- {id}: {pred_ids[id]} ↔ {label_ids[id]}")
                    if len(matched_ids) > 5:
                        st.write(f"... and {len(matched_ids) - 5} more")

                if pred_only:
                    st.write("\n**Predictions without labels:**")
                    for id in sorted(list(pred_only))[:5]:
                        st.write(f"- {id}: {pred_ids[id]}")
                    if len(pred_only) > 5:
                        st.write(f"... and {len(pred_only) - 5} more")

                if label_only:
                    st.write("\n**Labels without predictions:**")
                    for id in sorted(list(label_only))[:5]:
                        st.write(f"- {id}: {label_ids[id]}")
                    if len(label_only) > 5:
                        st.write(f"... and {len(label_only) - 5} more")

        # Scoring options
        st.write("### ⚙️ Scoring Options")
        col1, col2, col3 = st.columns(3)
        with col1:
            use_llm = st.checkbox("Use LLM for nuanced scoring", value=True)
            llm_model = st.selectbox(
                "LLM Model",
                ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
                disabled=not use_llm,
            )
        with col2:
            skip_errors = st.checkbox("Skip files with errors", value=True)
            save_results = st.checkbox("Save results to file", value=True)
        with col3:
            use_threading = st.checkbox(
                "Use multi-threading",
                value=True,
                help="Process multiple examples in parallel for faster scoring",
            )
            max_workers = st.number_input(
                "Number of threads",
                min_value=1,
                max_value=16,
                value=4,
                disabled=not use_threading,
            )

        # Score button
        if st.button(
            "🚀 Score All Matched Examples",
            type="primary",
            disabled=not (pred_dir and label_dir),
        ):
            if not (pred_files and label_files and matched_ids):
                st.error(
                    "No matching files found between predictions and labels directories"
                )
            else:
                # Initialize progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()
                results_placeholder = st.empty()

                # Initialize scorer
                llm_func = None
                if use_llm:
                    try:
                        client = get_openai_client()

                        def llm_func(prompt: str) -> str:
                            response = client.chat.completions.create(
                                model=llm_model,
                                messages=[
                                    {
                                        "role": "system",
                                        "content": "You are a helpful assistant that evaluates agent performance.",
                                    },
                                    {"role": "user", "content": prompt},
                                ],
                                temperature=0,
                                response_format={"type": "json_object"},
                            )
                            return response.choices[0].message.content

                        st.info(f"Using {llm_model} for scoring")
                    except Exception as e:
                        st.warning(f"LLM unavailable ({e}), using exact match scoring")
                        llm_func = None
                else:
                    st.info("Using exact match scoring")

                scorer = ChecklistScorer(llm_generate_func=llm_func)

                # Score all matched examples
                results = {}
                scores_list = {
                    "overall": [],
                    "retrieval": [],
                    "identification": [],
                    "task_selection": [],
                    # Add individual retrieval metrics for detailed analysis
                    "precision": [],
                    "recall": [],
                    "f1": [],
                }
                errors = []

                # Convert paths to Path objects
                pred_path = Path(pred_dir).expanduser()
                label_path = Path(label_dir).expanduser()
                input_path = Path(input_dir).expanduser() if input_dir else None

                # Function to process a single example
                def process_example(example_id):
                    """Process a single example and return results."""
                    try:
                        # Load prediction
                        pred_file = pred_path / pred_ids[example_id]
                        with open(pred_file, "r") as f:
                            pred_data = json.load(f)

                        # Convert to Annotation object
                        if "retrieved_document_ids" in pred_data:  # New format
                            annotation = Annotation.from_dict(pred_data)
                        elif (
                            "retrieved_documents" in pred_data
                        ):  # baseline_agent format
                            action_data = pred_data.get("action", {})
                            annotation = Annotation(
                                retrieved_document_ids=pred_data.get(
                                    "retrieved_documents", []
                                ),
                                bottleneck_description=pred_data.get("bottleneck", ""),
                                action_selection=ActionSelection(
                                    name=action_data.get("function_name", ""),
                                    schema=action_data.get("parameters", {}),
                                ),
                            )
                        else:
                            # Try to handle other formats
                            annotation = Annotation(
                                retrieved_document_ids=pred_data.get(
                                    "selected_ids", []
                                ),
                                bottleneck_description=pred_data.get("bottleneck", ""),
                                action_selection=ActionSelection(
                                    name=pred_data.get("action", {}).get("name", ""),
                                    schema=pred_data.get("action", {}).get(
                                        "parameters", {}
                                    ),
                                ),
                            )

                        # Load label
                        label_file = label_path / label_ids[example_id]
                        with open(label_file, "r") as f:
                            output_data = json.load(f)

                        # Load input if available
                        input_data = None
                        if input_path and input_path.exists():
                            input_file = input_path / f"{example_id}_input.json"
                            if input_file.exists():
                                with open(input_file, "r") as f:
                                    input_data = json.load(f)

                        # Score
                        scoring_result = scorer.score_annotation(
                            annotation, output_data, input_data
                        )

                        return example_id, scoring_result, None

                    except Exception as e:
                        return example_id, None, str(e)

                # Process examples with threading
                if use_threading and max_workers > 1:
                    with concurrent.futures.ThreadPoolExecutor(
                        max_workers=max_workers
                    ) as executor:
                        # Submit all tasks
                        future_to_id = {
                            executor.submit(process_example, example_id): example_id
                            for example_id in sorted(matched_ids)
                        }

                        # Process completed tasks
                        completed = 0
                        for future in concurrent.futures.as_completed(future_to_id):
                            completed += 1
                            progress = completed / len(matched_ids)
                            progress_bar.progress(progress)

                            example_id, scoring_result, error = future.result()
                            status_text.text(
                                f"Processed {example_id} ({completed}/{len(matched_ids)})"
                            )

                            if error:
                                error_msg = f"Error processing {example_id}: {error}"
                                errors.append(error_msg)
                                if not skip_errors:
                                    st.error(error_msg)
                                    break
                                results[example_id] = {"error": error}
                            else:
                                # Store results
                                results[example_id] = scoring_result.to_dict()
                                scores_list["overall"].append(
                                    scoring_result.overall_score
                                )
                                scores_list["retrieval"].append(
                                    scoring_result.retrieval_score
                                )
                                scores_list["identification"].append(
                                    scoring_result.identification_score
                                )
                                scores_list["task_selection"].append(
                                    scoring_result.task_selection_score
                                )

                                # Collect detailed retrieval metrics for aggregate analysis
                                retrieval_details = scoring_result.retrieval_details
                                if isinstance(retrieval_details, dict):
                                    scores_list["precision"].append(
                                        retrieval_details.get("precision", 0.0)
                                    )
                                    scores_list["recall"].append(
                                        retrieval_details.get("recall", 0.0)
                                    )
                                    scores_list["f1"].append(
                                        retrieval_details.get("f1_score", 0.0)
                                    )
                                else:
                                    # Fallback if retrieval_details is not available
                                    scores_list["precision"].append(0.0)
                                    scores_list["recall"].append(0.0)
                                    scores_list["f1"].append(0.0)
                else:
                    # Sequential processing (original code)
                    for idx, example_id in enumerate(sorted(matched_ids)):
                        progress = (idx + 1) / len(matched_ids)
                        progress_bar.progress(progress)
                        status_text.text(
                            f"Processing {example_id} ({idx + 1}/{len(matched_ids)})"
                        )

                        example_id, scoring_result, error = process_example(example_id)

                        if error:
                            error_msg = f"Error processing {example_id}: {error}"
                            errors.append(error_msg)
                            if not skip_errors:
                                st.error(error_msg)
                                break
                            results[example_id] = {"error": error}
                        else:
                            # Store results
                            results[example_id] = scoring_result.to_dict()
                            scores_list["overall"].append(scoring_result.overall_score)
                            scores_list["retrieval"].append(
                                scoring_result.retrieval_score
                            )
                            scores_list["identification"].append(
                                scoring_result.identification_score
                            )
                            scores_list["task_selection"].append(
                                scoring_result.task_selection_score
                            )

                            # Collect detailed retrieval metrics for aggregate analysis
                            retrieval_details = scoring_result.retrieval_details
                            if isinstance(retrieval_details, dict):
                                scores_list["precision"].append(
                                    retrieval_details.get("precision", 0.0)
                                )
                                scores_list["recall"].append(
                                    retrieval_details.get("recall", 0.0)
                                )
                                scores_list["f1"].append(
                                    retrieval_details.get("f1_score", 0.0)
                                )
                            else:
                                # Fallback if retrieval_details is not available
                                scores_list["precision"].append(0.0)
                                scores_list["recall"].append(0.0)
                                scores_list["f1"].append(0.0)

                # Clear progress
                progress_bar.empty()
                status_text.empty()

                # Calculate statistics
                def calculate_stats(scores: List[float]) -> Dict[str, float]:
                    if not scores:
                        return {
                            "mean": 0.0,
                            "std": 0.0,
                            "min": 0.0,
                            "max": 0.0,
                            "count": 0,
                        }

                    import numpy as np

                    return {
                        "mean": float(np.mean(scores)),
                        "std": float(np.std(scores)),
                        "min": float(np.min(scores)),
                        "max": float(np.max(scores)),
                        "count": len(scores),
                    }

                # Display results
                with results_placeholder.container():
                    st.success(
                        f"✅ Scoring complete! Processed {len(results)} examples."
                    )

                    if errors:
                        with st.expander(f"⚠️ Errors ({len(errors)})", expanded=False):
                            for error in errors:
                                st.write(f"- {error}")

                    # Aggregate statistics
                    st.write("### 📊 Aggregate Statistics")

                    stats = {
                        "overall": calculate_stats(scores_list["overall"]),
                        "retrieval": calculate_stats(scores_list["retrieval"]),
                        "identification": calculate_stats(
                            scores_list["identification"]
                        ),
                        "task_selection": calculate_stats(
                            scores_list["task_selection"]
                        ),
                        # Add detailed retrieval metrics statistics
                        "precision": calculate_stats(scores_list["precision"]),
                        "recall": calculate_stats(scores_list["recall"]),
                        "f1": calculate_stats(scores_list["f1"]),
                    }

                    # Display overall score prominently
                    col1, col2, col3 = st.columns([2, 1, 1])
                    with col1:
                        val = stats["overall"]["mean"]
                        color = (
                            "green" if val >= 0.7 else "orange" if val >= 0.4 else "red"
                        )
                        st.markdown("**Overall Score**")
                        st.markdown(
                            f"<span style='color:{color}; font-size:28px'>{val:.2%}</span>",
                            unsafe_allow_html=True,
                        )
                        st.caption(f"σ = {stats['overall']['std']:.2%}")

                    with col2:
                        st.metric(
                            "Identification",
                            f"{stats['identification']['mean']:.2%}",
                            f"σ = {stats['identification']['std']:.2%}",
                        )

                    with col3:
                        st.metric(
                            "Task Selection",
                            f"{stats['task_selection']['mean']:.2%}",
                            f"σ = {stats['task_selection']['std']:.2%}",
                        )

                    # Display detailed retrieval metrics in a beautiful layout
                    st.markdown("### 🔍 Retrieval Metrics")

                    # Create a visually appealing metrics display
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        # Overall retrieval (F1 score)
                        f1_val = stats["f1"]["mean"]
                        f1_color = (
                            "green"
                            if f1_val >= 0.7
                            else "orange" if f1_val >= 0.4 else "red"
                        )
                        st.markdown("**F1 Score**")
                        st.markdown(
                            f"<span style='color:{f1_color}; font-size:20px; font-weight:bold'>{f1_val:.2%}</span>",
                            unsafe_allow_html=True,
                        )
                        st.caption(f"σ = {stats['f1']['std']:.2%}")

                    with col2:
                        # Precision
                        prec_val = stats["precision"]["mean"]
                        prec_color = (
                            "green"
                            if prec_val >= 0.7
                            else "orange" if prec_val >= 0.4 else "red"
                        )
                        st.markdown("**Precision**")
                        st.markdown(
                            f"<span style='color:{prec_color}; font-size:20px; font-weight:bold'>{prec_val:.2%}</span>",
                            unsafe_allow_html=True,
                        )
                        st.caption(f"σ = {stats['precision']['std']:.2%}")

                    with col3:
                        # Recall
                        recall_val = stats["recall"]["mean"]
                        recall_color = (
                            "green"
                            if recall_val >= 0.7
                            else "orange" if recall_val >= 0.4 else "red"
                        )
                        st.markdown("**Recall**")
                        st.markdown(
                            f"<span style='color:{recall_color}; font-size:20px; font-weight:bold'>{recall_val:.2%}</span>",
                            unsafe_allow_html=True,
                        )
                        st.caption(f"σ = {stats['recall']['std']:.2%}")

                    with col4:
                        # Legacy retrieval score for comparison
                        st.markdown("**Legacy Score**")
                        st.markdown(
                            f"<span style='color:gray; font-size:16px'>{stats['retrieval']['mean']:.2%}</span>",
                            unsafe_allow_html=True,
                        )
                        st.caption(f"σ = {stats['retrieval']['std']:.2%}")

                    # Detailed results table
                    st.write("### 📋 Individual Results")

                    # Create DataFrame
                    df_data = []
                    for example_id, result in results.items():
                        if "error" not in result:
                            # Extract retrieval details for individual metrics
                            retrieval_details = result.get("retrieval_details", {})
                            precision = (
                                retrieval_details.get("precision", 0.0)
                                if isinstance(retrieval_details, dict)
                                else 0.0
                            )
                            recall = (
                                retrieval_details.get("recall", 0.0)
                                if isinstance(retrieval_details, dict)
                                else 0.0
                            )
                            f1_score = (
                                retrieval_details.get("f1_score", 0.0)
                                if isinstance(retrieval_details, dict)
                                else 0.0
                            )

                            df_data.append(
                                {
                                    "Example ID": example_id,
                                    "Overall": result["overall_score"],
                                    "F1": f1_score,
                                    "Precision": precision,
                                    "Recall": recall,
                                    "Identification": result["identification_score"],
                                    "Task Selection": result["task_selection_score"],
                                }
                            )

                    if df_data:
                        df = pd.DataFrame(df_data)

                        # Sort by overall score
                        df = df.sort_values("Overall", ascending=False)

                        # Display with formatting
                        styled_df = df.style.format(
                            {
                                "Overall": "{:.2%}",
                                "F1": "{:.2%}",
                                "Precision": "{:.2%}",
                                "Recall": "{:.2%}",
                                "Identification": "{:.2%}",
                                "Task Selection": "{:.2%}",
                            }
                        )

                        # Try to add background gradient if matplotlib is available
                        try:
                            # Apply gradient to multiple columns for better visualization
                            styled_df = styled_df.background_gradient(
                                subset=["Overall", "F1", "Precision", "Recall"],
                                cmap="RdYlGn",
                                vmin=0,
                                vmax=1,
                            )
                        except ImportError:
                            # Matplotlib not available, skip gradient
                            pass

                    st.dataframe(styled_df, use_container_width=True)

                # Prepare full results
                full_results = {
                    "metadata": {
                        "predictions_directory": pred_dir,
                        "labels_directory": label_dir,
                        "input_directory": input_dir or "not_used",
                        "timestamp": datetime.now().isoformat(),
                        "total_examples": len(matched_ids),
                        "successful": len(
                            [r for r in results.values() if "error" not in r]
                        ),
                        "errors": len(errors),
                        "llm_model": llm_model if use_llm else "exact_match",
                    },
                    "aggregate_statistics": stats,
                    "individual_results": results,
                }

                # Save/download results
                col1, col2 = st.columns(2)

                with col1:
                    # Download button
                    st.download_button(
                        "💾 Download Full Results (JSON)",
                        data=json.dumps(full_results, indent=2),
                        file_name=f"batch_scoring_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                    )

                with col2:
                    # Download CSV
                    if df_data:
                        csv = df.to_csv(index=False)
                        st.download_button(
                            "📊 Download Results (CSV)",
                            data=csv,
                            file_name=f"batch_scoring_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                        )

                    # Optionally save to file
                    if save_results:
                        output_file = (
                            Path(pred_dir).parent
                            / f"scoring_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                        )
                        try:
                            with open(output_file, "w") as f:
                                json.dump(full_results, f, indent=2)
                            st.info(f"Results saved to: {output_file}")
                        except Exception as e:
                            st.warning(f"Could not save results file: {e}")

elif page == "LLM Scoring Validation":
    st.title("LLM Scoring Validation")
    st.write(
        "Compare human annotations against LLM scoring outputs for bottleneck identification and task parameters."
    )

    # Add tabs for different functions
    tab1, tab2, tab3 = st.tabs(
        ["Generate LLM Scoring", "Human Annotation", "Compare & Analyze"]
    )

    with tab1:
        st.subheader("🤖 Generate LLM Scoring Outputs")
        st.write(
            "Run LLM scoring on model predictions to generate outputs for human validation."
        )

        col1, col2 = st.columns(2)

        with col1:
            st.write("**Predictions Directory**")
            predictions_dir = st.text_input(
                "Path to model predictions",
                placeholder="e.g., ~/Downloads/baseline_gpt_5",
                help="Directory containing model prediction files in JSON format",
            )

            st.write("**Labels Directory**")
            labels_dir = st.text_input(
                "Path to ground truth labels",
                placeholder="e.g., batch_examples/outputs/batch_name",
                help="Directory containing ground truth output files",
            )

        with col2:
            st.write("**Output Directory**")
            llm_output_dir = st.text_input(
                "Path to save LLM scoring outputs",
                placeholder="e.g., llm_scoring_outputs",
                help="Directory where LLM scoring outputs will be saved",
            )

            st.write("**Input Directory (Optional)**")
            input_dir = st.text_input(
                "Path to input files",
                placeholder="e.g., batch_examples/inputs/batch_name",
                help="Optional: Input files for additional context",
            )

        # Options
        col1, col2 = st.columns(2)
        with col1:
            llm_model = st.selectbox(
                "LLM Model for Scoring",
                ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
                index=0,
            )
        with col2:
            skip_errors = st.checkbox("Skip files with errors", value=True)

        # Generate button
        if st.button("🚀 Generate LLM Scoring", type="primary"):
            if not (predictions_dir and labels_dir and llm_output_dir):
                st.error("Please provide predictions, labels, and output directories")
            else:
                try:
                    from evaluation.generate_llm_scoring import (
                        generate_llm_scoring_for_directory,
                    )

                    with st.spinner("Generating LLM scoring outputs..."):
                        summary = generate_llm_scoring_for_directory(
                            Path(predictions_dir),
                            Path(labels_dir),
                            Path(llm_output_dir),
                            Path(input_dir) if input_dir else None,
                            llm_model,
                            skip_errors,
                        )

                    st.success(
                        f"✅ Generated LLM scoring for {summary['metadata']['processed']} examples!"
                    )

                    # Show summary
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Processed", summary["metadata"]["processed"])
                    with col2:
                        st.metric("Matched", summary["metadata"]["total_matched"])
                    with col3:
                        st.metric("Errors", summary["metadata"]["errors"])

                    if summary["errors"]:
                        with st.expander("⚠️ Errors", expanded=False):
                            for error in summary["errors"]:
                                st.write(f"- {error}")

                    # Download summary
                    st.download_button(
                        "📥 Download Summary",
                        data=json.dumps(summary, indent=2),
                        file_name=f"llm_scoring_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                    )

                except Exception as e:
                    st.error(f"❌ Error generating LLM scoring: {e}")
                    import traceback

                    st.error(traceback.format_exc())

    with tab2:
        st.subheader("👤 Human Annotation Interface")
        st.write(
            "Annotate bottleneck identification and task parameters to compare against LLM scoring."
        )

        # Load LLM scoring outputs
        st.write("### Load LLM Scoring Outputs")
        llm_scoring_dir = st.text_input(
            "LLM Scoring Directory",
            placeholder="e.g., llm_scoring_outputs",
            help="Directory containing LLM scoring output files",
        )

        # Load prediction directory for context
        pred_context_dir = st.text_input(
            "Predictions Directory (for context)",
            placeholder="e.g., ~/Downloads/baseline_gpt_5",
            help="Directory with original model predictions for context",
        )

        if llm_scoring_dir and pred_context_dir:
            llm_dir = Path(llm_scoring_dir)
            pred_dir = Path(pred_context_dir)

            if llm_dir.exists() and pred_dir.exists():
                # List available examples
                llm_files = list(llm_dir.glob("*_llm_scoring.json"))
                if llm_files:
                    # Extract example IDs
                    example_ids = [
                        f.name.replace("_llm_scoring.json", "") for f in llm_files
                    ]

                    selected_example = st.selectbox(
                        "Select Example to Annotate", example_ids
                    )

                    if selected_example:
                        # Load LLM scoring data
                        llm_file = llm_dir / f"{selected_example}_llm_scoring.json"
                        try:
                            with open(llm_file, "r") as f:
                                llm_data = json.load(f)

                            llm_scoring = llm_data.get("llm_scoring", {})

                            # Find corresponding prediction file
                            pred_files = list(
                                pred_dir.glob(f"{selected_example}*.json")
                            )
                            pred_file = pred_files[0] if pred_files else None

                            pred_data = None
                            if pred_file:
                                with open(pred_file, "r") as f:
                                    pred_data = json.load(f)

                            # Display context
                            st.write("### 📋 Context")

                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**Model Prediction:**")
                                if pred_data:
                                    # Show model's bottleneck and action
                                    bottleneck = pred_data.get(
                                        "bottleneck",
                                        pred_data.get("bottleneck_description", ""),
                                    )
                                    if bottleneck:
                                        st.info(f"**Bottleneck:** {bottleneck}")

                                    action = pred_data.get("action", {})
                                    if action:
                                        action_name = action.get(
                                            "function_name", action.get("name", "")
                                        )
                                        if action_name:
                                            st.info(f"**Action:** {action_name}")

                            with col2:
                                st.write("**LLM Expected (Ground Truth):**")
                                # Show what LLM considered as ground truth
                                if "bottleneck_identification" in llm_scoring:
                                    expected_bottleneck = llm_scoring[
                                        "bottleneck_identification"
                                    ].get("expected", "")
                                    if expected_bottleneck:
                                        st.success(
                                            f"**Expected Bottleneck:** {expected_bottleneck}"
                                        )

                                if "action_selection" in llm_scoring:
                                    expected_action = llm_scoring[
                                        "action_selection"
                                    ].get("expected_action", "")
                                    if expected_action:
                                        st.success(
                                            f"**Expected Action:** {expected_action}"
                                        )

                            # Human annotation interface
                            st.write("### 🎯 Your Annotations")

                            # Bottleneck identification annotation
                            if "bottleneck_identification" in llm_scoring:
                                st.write("#### Bottleneck Identification")

                                # Show LLM's judgment
                                llm_judgment = llm_scoring[
                                    "bottleneck_identification"
                                ].get("judgment", "UNKNOWN")
                                llm_reasoning = llm_scoring[
                                    "bottleneck_identification"
                                ].get("reasoning", "")

                                with st.expander("LLM Judgment", expanded=True):
                                    color = {
                                        "CORRECT": "green",
                                        "PARTIALLY_CORRECT": "orange",
                                        "INCORRECT": "red",
                                    }.get(llm_judgment, "gray")
                                    st.markdown(
                                        f"**LLM Judgment:** <span style='color:{color}'>{llm_judgment}</span>",
                                        unsafe_allow_html=True,
                                    )
                                    if llm_reasoning:
                                        st.write(f"**LLM Reasoning:** {llm_reasoning}")

                                # Human annotation
                                human_bottleneck_judgment = st.radio(
                                    "Your judgment on bottleneck identification:",
                                    ["CORRECT", "PARTIALLY_CORRECT", "INCORRECT"],
                                    key="human_bottleneck",
                                )

                                human_bottleneck_reasoning = st.text_area(
                                    "Your reasoning:",
                                    placeholder="Explain why you agree or disagree with the model's bottleneck identification...",
                                    key="human_bottleneck_reasoning",
                                )

                            # Task parameters annotation
                            if "task_parameters" in llm_scoring:
                                st.write("#### Task Parameters")

                                # Show LLM's judgment
                                llm_param_judgment = llm_scoring["task_parameters"].get(
                                    "judgment", "UNKNOWN"
                                )
                                llm_param_reasoning = llm_scoring[
                                    "task_parameters"
                                ].get("reasoning", "")

                                with st.expander(
                                    "LLM Judgment on Parameters", expanded=True
                                ):
                                    color = {
                                        "CORRECT": "green",
                                        "PARTIALLY_CORRECT": "orange",
                                        "INCORRECT": "red",
                                    }.get(llm_param_judgment, "gray")
                                    st.markdown(
                                        f"**LLM Judgment:** <span style='color:{color}'>{llm_param_judgment}</span>",
                                        unsafe_allow_html=True,
                                    )
                                    if llm_param_reasoning:
                                        st.write(
                                            f"**LLM Reasoning:** {llm_param_reasoning}"
                                        )

                                    # Show selected vs expected parameters
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.write("**Model Selected:**")
                                        selected_params = llm_scoring[
                                            "task_parameters"
                                        ].get("selected", {})
                                        if selected_params:
                                            st.json(selected_params)

                                    with col2:
                                        st.write("**Expected:**")
                                        expected_params = llm_scoring[
                                            "task_parameters"
                                        ].get("expected", {})
                                        if expected_params:
                                            st.json(expected_params)

                                # Human annotation
                                human_param_judgment = st.radio(
                                    "Your judgment on task parameters:",
                                    ["CORRECT", "PARTIALLY_CORRECT", "INCORRECT"],
                                    key="human_params",
                                )

                                human_param_reasoning = st.text_area(
                                    "Your reasoning:",
                                    placeholder="Explain why you agree or disagree with the LLM's assessment of the parameters...",
                                    key="human_param_reasoning",
                                )

                            # Annotator info
                            annotator_name = st.text_input(
                                "Annotator Name",
                                value="Anonymous",
                                help="Enter your name for tracking",
                            )

                            # Save annotation
                            if st.button("💾 Save Human Annotation", type="primary"):
                                # Create human annotation
                                human_annotation = {
                                    "example_id": selected_example,
                                    "annotator": annotator_name,
                                    "timestamp": datetime.now().isoformat(),
                                }

                                if "bottleneck_identification" in llm_scoring:
                                    human_annotation["bottleneck_identification"] = {
                                        "judgment": human_bottleneck_judgment,
                                        "reasoning": human_bottleneck_reasoning,
                                        "llm_judgment": llm_judgment,
                                        "agreement": human_bottleneck_judgment
                                        == llm_judgment,
                                    }

                                if "task_parameters" in llm_scoring:
                                    human_annotation["task_parameters"] = {
                                        "judgment": human_param_judgment,
                                        "reasoning": human_param_reasoning,
                                        "llm_judgment": llm_param_judgment,
                                        "agreement": human_param_judgment
                                        == llm_param_judgment,
                                    }

                                # Save to annotations directory
                                annotations_dir = llm_dir / "human_annotations"
                                annotations_dir.mkdir(exist_ok=True)

                                annotation_file = (
                                    annotations_dir
                                    / f"{selected_example}_human_annotation.json"
                                )
                                with open(annotation_file, "w") as f:
                                    json.dump(human_annotation, f, indent=2)

                                st.success(
                                    f"✅ Human annotation saved: {annotation_file.name}"
                                )

                                # Download option
                                st.download_button(
                                    "📥 Download Annotation",
                                    data=json.dumps(human_annotation, indent=2),
                                    file_name=annotation_file.name,
                                    mime="application/json",
                                )

                        except Exception as e:
                            st.error(f"Error loading LLM scoring data: {e}")

                else:
                    st.warning("No LLM scoring files found in the directory")
            else:
                if not llm_dir.exists():
                    st.warning("LLM scoring directory not found")
                if not pred_dir.exists():
                    st.warning("Predictions directory not found")

    with tab3:
        st.subheader("📊 Compare & Analyze Agreement")
        st.write("Analyze agreement between human annotations and LLM scoring.")

        # Directory selection
        analysis_llm_dir = st.text_input(
            "LLM Scoring Directory (with human annotations)",
            placeholder="e.g., llm_scoring_outputs",
            help="Directory containing both LLM scoring outputs and human annotations",
        )

        if analysis_llm_dir and st.button("📈 Analyze Agreement"):
            llm_dir = Path(analysis_llm_dir)
            human_annotations_dir = llm_dir / "human_annotations"

            if not human_annotations_dir.exists():
                st.error(
                    "No human annotations found. Please annotate some examples first."
                )
            else:
                # Load all human annotations
                human_files = list(
                    human_annotations_dir.glob("*_human_annotation.json")
                )

                if not human_files:
                    st.warning("No human annotation files found")
                else:
                    st.success(f"Found {len(human_files)} human annotations")

                    # Calculate agreement statistics
                    bottleneck_agreements = []
                    parameter_agreements = []
                    all_annotations = []

                    for human_file in human_files:
                        try:
                            with open(human_file, "r") as f:
                                annotation = json.load(f)

                            all_annotations.append(annotation)

                            # Check bottleneck agreement
                            if "bottleneck_identification" in annotation:
                                bottleneck_agreements.append(
                                    annotation["bottleneck_identification"]["agreement"]
                                )

                            # Check parameter agreement
                            if "task_parameters" in annotation:
                                parameter_agreements.append(
                                    annotation["task_parameters"]["agreement"]
                                )

                        except Exception as e:
                            st.error(f"Error loading {human_file.name}: {e}")

                    # Display aggregate statistics
                    st.write("### 📊 Agreement Statistics")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        if bottleneck_agreements:
                            bottleneck_agreement_rate = sum(
                                bottleneck_agreements
                            ) / len(bottleneck_agreements)
                            st.metric(
                                "Bottleneck Identification Agreement",
                                f"{bottleneck_agreement_rate:.1%}",
                                f"{sum(bottleneck_agreements)}/{len(bottleneck_agreements)}",
                            )
                        else:
                            st.metric("Bottleneck Identification Agreement", "N/A")

                    with col2:
                        if parameter_agreements:
                            parameter_agreement_rate = sum(parameter_agreements) / len(
                                parameter_agreements
                            )
                            st.metric(
                                "Task Parameters Agreement",
                                f"{parameter_agreement_rate:.1%}",
                                f"{sum(parameter_agreements)}/{len(parameter_agreements)}",
                            )
                        else:
                            st.metric("Task Parameters Agreement", "N/A")

                    with col3:
                        all_agreements = bottleneck_agreements + parameter_agreements
                        if all_agreements:
                            overall_agreement_rate = sum(all_agreements) / len(
                                all_agreements
                            )
                            st.metric(
                                "Overall Agreement",
                                f"{overall_agreement_rate:.1%}",
                                f"{sum(all_agreements)}/{len(all_agreements)}",
                            )
                        else:
                            st.metric("Overall Agreement", "N/A")

                    # Detailed breakdown table
                    if all_annotations:
                        st.write("### 📋 Detailed Results")

                        # Create DataFrame for display
                        table_data = []
                        for annotation in all_annotations:
                            row = {
                                "Example ID": annotation["example_id"],
                                "Annotator": annotation["annotator"],
                            }

                            # Bottleneck data
                            if "bottleneck_identification" in annotation:
                                bot_data = annotation["bottleneck_identification"]
                                row["Bottleneck - LLM"] = bot_data.get(
                                    "llm_judgment", "N/A"
                                )
                                row["Bottleneck - Human"] = bot_data.get(
                                    "judgment", "N/A"
                                )
                                row["Bottleneck - Agree"] = (
                                    "✅" if bot_data.get("agreement") else "❌"
                                )
                            else:
                                row["Bottleneck - LLM"] = "N/A"
                                row["Bottleneck - Human"] = "N/A"
                                row["Bottleneck - Agree"] = "N/A"

                            # Parameter data
                            if "task_parameters" in annotation:
                                param_data = annotation["task_parameters"]
                                row["Parameters - LLM"] = param_data.get(
                                    "llm_judgment", "N/A"
                                )
                                row["Parameters - Human"] = param_data.get(
                                    "judgment", "N/A"
                                )
                                row["Parameters - Agree"] = (
                                    "✅" if param_data.get("agreement") else "❌"
                                )
                            else:
                                row["Parameters - LLM"] = "N/A"
                                row["Parameters - Human"] = "N/A"
                                row["Parameters - Agree"] = "N/A"

                            table_data.append(row)

                        if table_data:
                            df = pd.DataFrame(table_data)
                            st.dataframe(df, use_container_width=True)

                            # Download detailed results
                            csv_data = df.to_csv(index=False)
                            st.download_button(
                                "📊 Download Results (CSV)",
                                data=csv_data,
                                file_name=f"llm_human_agreement_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                mime="text/csv",
                            )

                            # Download full annotations
                            full_data = {
                                "metadata": {
                                    "timestamp": datetime.now().isoformat(),
                                    "total_annotations": len(all_annotations),
                                    "bottleneck_agreement_rate": (
                                        bottleneck_agreement_rate
                                        if bottleneck_agreements
                                        else None
                                    ),
                                    "parameter_agreement_rate": (
                                        parameter_agreement_rate
                                        if parameter_agreements
                                        else None
                                    ),
                                    "overall_agreement_rate": (
                                        overall_agreement_rate
                                        if all_agreements
                                        else None
                                    ),
                                },
                                "annotations": all_annotations,
                            }

                            st.download_button(
                                "📥 Download Full Analysis (JSON)",
                                data=json.dumps(full_data, indent=2),
                                file_name=f"llm_human_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                mime="application/json",
                            )


# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**PersonaSim Evaluation System**")
st.sidebar.markdown("v2.1.0 | Built with Streamlit")

#!/bin/bash
# LLM Ablation Study - Evaluation
# Evaluates inference results from each dataset
#
# This script compares model predictions against ground truth labels
# for datasets generated by different LLMs.

set -e

echo "=========================================="
echo "PROBE LLM Ablation Study - Evaluation"
echo "=========================================="
echo ""

# Configuration
INFERENCE_MODEL="gpt-5"
USE_LLM_JUDGE=true  # Set to false for exact matching only

# Data generation models
DATA_GEN_MODELS=("gpt5mini" "gpt41" "claude")

echo "Evaluating results for each dataset..."
if [ "$USE_LLM_JUDGE" = true ]; then
    echo "Using LLM judge for semantic matching"
else
    echo "Using exact matching only"
fi
echo ""

for model in "${DATA_GEN_MODELS[@]}"; do
    # Find the latest generated data directory (full batch path)
    BATCH_DIR=$(find generated_data -type d -path "*ablation_${model}*batch" 2>/dev/null | sort -r | head -1)
    
    if [ -z "$BATCH_DIR" ]; then
        echo "⚠️  No batch directory found for ablation_${model}, skipping..."
        continue
    fi
    
    # Extract batch name for finding predictions
    BATCH_NAME=$(basename "$BATCH_DIR")
    
    # Set up paths - predictions are moved to batch_name_model after inference
    LABELS_DIR="${BATCH_DIR}/outputs"
    INPUTS_DIR="${BATCH_DIR}/inputs"
    PREDICTIONS_DIR="results/${BATCH_NAME}_${model}"
    OUTPUT_FILE="results/${BATCH_NAME}_${model}/evaluation_results.json"
    
    # Check if required directories exist
    if [ ! -d "$LABELS_DIR" ]; then
        echo "⚠️  No outputs directory found at $LABELS_DIR, skipping..."
        continue
    fi
    
    if [ ! -d "$PREDICTIONS_DIR" ]; then
        echo "⚠️  No predictions directory found at $PREDICTIONS_DIR, skipping..."
        echo "    Make sure inference has completed and results are saved."
        continue
    fi
    
    echo ""
    echo "▶ Evaluating results for data generated by: $model"
    echo "  Batch directory: $BATCH_DIR"
    echo "  Predictions: $PREDICTIONS_DIR"
    echo "  Labels: $LABELS_DIR"
    echo "  Inputs: $INPUTS_DIR"
    echo ""
    
    # Build evaluation command
    EVAL_CMD="PYTHONPATH=\"\${PYTHONPATH:+\${PYTHONPATH}:}\$(pwd)\" doppler run -- python evaluation/batch_evaluate_baselines.py \
        --predictions-dir \"$PREDICTIONS_DIR\" \
        --labels-dir \"$LABELS_DIR\" \
        --inputs-dir \"$INPUTS_DIR\" \
        --output-file \"$OUTPUT_FILE\""
    
    # Add LLM judge flag if enabled
    if [ "$USE_LLM_JUDGE" = true ]; then
        EVAL_CMD="$EVAL_CMD --use-llm-judge"
    fi
    
    # Run evaluation
    eval $EVAL_CMD
    
    echo "✓ Evaluation complete for: $model"
    echo "  Results saved to: $OUTPUT_FILE"
    echo ""
done

echo ""
echo "=========================================="
echo "Evaluation Complete!"
echo "=========================================="
echo ""
echo "Results saved to: results/<batch_timestamp>_<model>/evaluation_results.json"
echo ""
echo "Next: Analyze results to compare model performance"
echo ""

